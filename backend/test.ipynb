{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreykhalov/anaconda3/envs/mipt_advanceML/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель загрузили\n",
      "Загружено 9846 уникальных ответов из базы.\n",
      "Интерактивный режим ретривера. Для выхода введите 'exit'.\n",
      "Michaels Response: Hi, Jan!\n",
      "Схожесть: 0.9249107975508943\n",
      "Michaels Response: How are you doing?\n",
      "Схожесть: 0.9585378099987611\n",
      "Michaels Response: What's going on?\n",
      "Схожесть: 0.9580179437172358\n",
      "Michaels Response: OK\n",
      "Схожесть: 0.7622931153529713\n",
      "Выход из интерактивного режима.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dill as pickle\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from torch.utils.data import Dataset as torchDataset\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def mean_pool(token_embeds: torch.tensor, attention_mask: torch.tensor) -> torch.tensor:\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(token_embeds.size()).float()\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
    "    return pool\n",
    "\n",
    "\n",
    "'''\n",
    "    model.eval(): Переводит модель в режим оценки (выключает тренировочные функции, такие как dropout).\n",
    "\n",
    "    tokenizer(...): Токенизирует входные тексты, устанавливая максимальную длину 128, добавляя паддинг и\n",
    "    обрезая длинные тексты. Результат возвращается в формате тензоров PyTorch.\n",
    "\n",
    "    model(...): Применяет модель к токенизированным входным данным\n",
    "    (идентификаторы токенов и маски внимания), получает выходные эмбеддинги (последние скрытые состояния).\n",
    "\n",
    "    mean_pool(...): Применяет функцию усреднения к эмбеддингам, используя маску внимания, чтобы получить\n",
    "    агрегированные эмбеддинги.\n",
    "\n",
    "    return pooled_embeds: Возвращает усредненные эмбеддинги для дальнейшего использования.\n",
    "'''\n",
    "\n",
    "def encode(input_texts: list[str], tokenizer: AutoTokenizer, model: AutoModel, device: str = \"cpu\"\n",
    ") -> torch.tensor:\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    tokenized_texts = tokenizer(input_texts, max_length=128,\n",
    "                                padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    token_embeds = model(tokenized_texts[\"input_ids\"].to(device),\n",
    "                         tokenized_texts[\"attention_mask\"].to(device)).last_hidden_state\n",
    "    pooled_embeds = mean_pool(token_embeds, tokenized_texts[\"attention_mask\"].to(device))\n",
    "    return pooled_embeds\n",
    "\n",
    "\n",
    "class Sbert(torch.nn.Module):\n",
    "    def __init__(self, max_length: int = 256, device=None):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.device = device if device is not None else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bert_model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.bert_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.linear = torch.nn.Linear(self.bert_model.config.hidden_size * 3, 1) # у нас бинарная классификация поэтому 1 нейрон\n",
    "\n",
    "    def forward(self, data: datasets.arrow_dataset.Dataset) -> torch.tensor:\n",
    "        request_input_ids = data[\"request_input_ids\"].to(self.device)\n",
    "        request_attention_mask = data[\"request_attention_mask\"].to(self.device)\n",
    "        responce_input_ids = data[\"responce_input_ids\"].to(self.device)\n",
    "        responce_attention_mask = data[\"responce_attention_mask\"].to(self.device)\n",
    "\n",
    "        \"\"\"\n",
    "    out_request = self.bert_model(...): Применяет модель BERT к входным данным (идентификаторы токенов и маска внимания) для вопроса, получая выходной объект.\n",
    "\n",
    "    out_responce = self.bert_model(...): Применяет ту же модель BERT к входным данным для ответа.\n",
    "\n",
    "    request_embeds = out_request.last_hidden_state: Извлекает последние скрытые состояния (эмбеддинги) для вопроса.\n",
    "\n",
    "    responce_embeds = out_responce.last_hidden_state: Извлекает последние скрытые состояния (эмбеддинги) для ответа.\n",
    "\n",
    "        \"\"\"\n",
    "        out_request = self.bert_model(request_input_ids, request_attention_mask)\n",
    "        out_responce = self.bert_model(responce_input_ids, responce_attention_mask)\n",
    "        request_embeds = out_request.last_hidden_state\n",
    "        responce_embeds = out_responce.last_hidden_state\n",
    "\n",
    "        \"\"\"\n",
    "    pooled_request_embeds = mean_pool(...): Усредняет эмбеддинги вопроса с учетом маски внимания, получая агрегированные эмбеддинги.\n",
    "\n",
    "    pooled_responce_embeds = mean_pool(...): Усредняет эмбеддинги ответа аналогично.\n",
    "\n",
    "    torch.cat([...], dim=-1): Объединяет (конкатенирует) три компонента:\n",
    "        Усредненные эмбеддинги вопроса.\n",
    "        Усредненные эмбеддинги ответа.\n",
    "        Абсолютная разница между усредненными эмбеддингами вопроса и ответа.\n",
    "        \"\"\"\n",
    "        pooled_request_embeds = mean_pool(request_embeds, request_attention_mask)\n",
    "        pooled_responce_embeds = mean_pool(responce_embeds, responce_attention_mask)\n",
    "\n",
    "        embeds =  torch.cat([pooled_request_embeds, pooled_responce_embeds,\n",
    "                             torch.abs(pooled_request_embeds - pooled_responce_embeds)],\n",
    "                            dim=-1)\n",
    "        return self.linear(embeds)\n",
    "\n",
    "\n",
    "# загружаем модель\n",
    "\n",
    "# Предполагается, что device уже определён (например, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "model_path = \"michael_scott_model.bin\"\n",
    "\n",
    "# назначаем устройство\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Создаём экземпляр модели\n",
    "model = Sbert().to(device)\n",
    "\n",
    "print(\"Модель загрузили\")\n",
    "\n",
    "# Загружаем веса из сохранённого файла\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  # переводим модель в режим инференса\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# теперь загрузим датасет с эмбедингами\n",
    "updated_dataset = load_dataset(\"Zamza/michael_scott_responces_NLP_emb\")\n",
    "\n",
    "class UniqueResponseRetriever:\n",
    "    \"\"\"\n",
    "    Ретривер, который получает датасет с эмбеддингами и оставляет только уникальные ответы.\n",
    "    Затем по входному запросу вычисляет его эмбеддинг (с использованием FT модели) и ищет по косинусной близости\n",
    "    наиболее похожий ответ из базы.\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_dataset, model, tokenizer, max_length: int = 256, device=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          hf_dataset: объект Hugging Face Dataset, содержащий как минимум колонки:\n",
    "                      \"response\" и \"ft_emb_responce\"\n",
    "          model: дообученная FT модель (экземпляр Sbert)\n",
    "          tokenizer: токенайзер, соответствующий модели\n",
    "          max_length: максимальная длина последовательности для токенизации\n",
    "          device: устройство для вычислений (если None, определяется автоматически)\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "            \n",
    "        self.model = model.to(self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Преобразуем датасет в pandas DataFrame и оставляем только уникальные ответы\n",
    "        dobj = hf_dataset['train']\n",
    "        df = dobj.to_pandas()\n",
    "        df_unique = df.drop_duplicates(subset=[\"response\"])\n",
    "        self.responses = df_unique[\"response\"].tolist()\n",
    "        # Предполагается, что \"ft_emb_responce\" хранится как список чисел для каждого примера\n",
    "        self.embeddings = np.vstack(df_unique[\"ft_emb_responce\"].apply(np.array).tolist())\n",
    "        print(f\"Загружено {len(self.responses)} уникальных ответов из базы.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Усредняет эмбеддинги токенов с учётом маски внимания.\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output.last_hidden_state  # shape: (batch_size, seq_len, hidden_size)\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def embed_text(self, text: str) -> np.array:\n",
    "        \"\"\"\n",
    "        Вычисляет эмбеддинг для данного текста с использованием токенайзера и FT модели.\n",
    "        \"\"\"\n",
    "        encoded = self.tokenizer(text, padding=\"max_length\", truncation=True,\n",
    "                                 max_length=self.max_length, return_tensors=\"pt\")\n",
    "        encoded = {k: v.to(self.device) for k, v in encoded.items()}\n",
    "        with torch.no_grad():\n",
    "            output = self.model.bert_model(**encoded)\n",
    "            emb = self.mean_pooling(output, encoded[\"attention_mask\"])\n",
    "        return emb.cpu().numpy()  # shape: (1, D)\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 1) -> tuple[list[str], np.array]:\n",
    "        \"\"\"\n",
    "        По входному запросу вычисляет его эмбеддинг, затем ищет top_k ответов из базы\n",
    "        с наибольшей косинусной схожестью.\n",
    "        Returns:\n",
    "          - Список найденных ответов.\n",
    "          - Соответствующие значения косинусной схожести.\n",
    "        \"\"\"\n",
    "        query_emb = self.embed_text(query)  # shape: (1, D)\n",
    "        similarities = cosine_similarity(query_emb, self.embeddings).flatten()\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        return [self.responses[i] for i in top_indices], similarities[top_indices]\n",
    "\n",
    "def interactive_mode(retriever: UniqueResponseRetriever):\n",
    "    \"\"\"\n",
    "    Бесконечный цикл, который принимает пользовательский ввод (до ввода \"exit\")\n",
    "    и выводит ответ от ретривера.\n",
    "    \"\"\"\n",
    "    print(\"Интерактивный режим ретривера. Для выхода введите 'exit'.\")\n",
    "    while True:\n",
    "        query = input(\">> \").strip()\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Выход из интерактивного режима.\")\n",
    "            break\n",
    "        resp, sim = retriever.retrieve(query, top_k=1)\n",
    "        print(\"Michaels Response:\", resp[0])\n",
    "        print(\"Схожесть:\", sim[0])\n",
    "\n",
    "# Создаем экземпляр ретривера на базе уникальных ответов\n",
    "retriever = UniqueResponseRetriever(hf_dataset=updated_dataset,\n",
    "                                    model=model,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    max_length=256,\n",
    "                                    device=device)\n",
    "\n",
    "retriever.model.to(device)\n",
    "retriever.model.eval()\n",
    "\n",
    "# Запускаем интерактивный режим ретривера (ТЕСТ!)\n",
    "interactive_mode(retriever)\n",
    "\n",
    "# ура он завелся!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mipt_advanceML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
